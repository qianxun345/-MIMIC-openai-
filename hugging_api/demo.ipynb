{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## langchain.embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# text = \"\"\n",
    "# doc_embedding = embeddings.embed_documents(text)\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, CSVLoader\n",
    "loader = CSVLoader('../unzip_data/a.csv')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "768\n",
      "0.042400114238262177\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "t = embeddings.embed_documents([t.page_content for t in documents])\n",
    "print(len(t))\n",
    "print(len(t[0]))\n",
    "print(t[0][0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "index_name = \"csv\"\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"6664c379-ef4f-41fb-afc4-7242589d2c07\",\n",
    "    environment=\"asia-southeast1-gcp-free\"\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=768)\n",
    "# else:\n",
    "#     pinecone.delete_index(index_name)\n",
    "#     pass\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in tqdm(documents)], embeddings, index_name=index_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1843 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "831a2f15a5694ca589a4571ddd477f77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    i_end = min(i+batch_size, len(documents))\n",
    "    vector_id = [str(j) for j in range(i, i_end)]\n",
    "    embedding_words = embeddings.embed_documents([t.page_content for t in documents[i:i_end]])\n",
    "    texts = [{'text': t.page_content} for t in documents[i:i_end]]\n",
    "    to_upsert = zip(vector_id, embedding_words, texts)\n",
    "    index.upsert(vectors=list(to_upsert))\n",
    "\n",
    "\n",
    "    # docsearch = Pinecone.from_texts([t.page_content for t in tqdm(documents[i:i+batch_size])], embeddings, index_name=index_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nres -> results -> [0]\\n    -> matches -> [id, metadata, score]\\n        -> metadata -> text\\n'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is the DISCHTIME going to be when his/her ADMITTIME is 2157-10-18 19:34:00 and its DIAGNOSIS is CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS GRAFT /SDA?\"\n",
    "query = \"what is the DISCHTIME going to be when his/her ADMITTIME is 2153-09-03 07:15:00 and its DIAGNOSIS is CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS GRAFT /SDA?\"\n",
    "\n",
    "find_index = index.query(queries=[embeddings.embed_documents([query])], top_k=15, include_metadata=True)\n",
    "'''\n",
    "res -> results -> [0]\n",
    "    -> matches -> [id, metadata, score]\n",
    "        -> metadata -> text\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "find_text = [r['metadata']['text'] for r in find_index['results'][0]['matches']]\n",
    "# for i in range(len(find_text)):\n",
    "#     print(find_text[i].split('\\n')[16])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# prompt = f\"From all the csv file,some patient information is as follows:{find_text}. Could you brainstorm three distinct solutions? Please consider a variety of factors.three different experts are answering this question. All experts will write down 1 step of their thinking,then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is {query}\"\n",
    "\n",
    "prompt = f\"From all the csv file,some patient information is as follows:{find_text}. I have a problem related to {query}. Could you brainstorm three distinct solutions? Please consider a variety of factors such as ADMITTIME, DIAGNOSIS and DISCHTIME.For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors.Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mServiceUnavailableError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NotionGpt\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[1;32m~\\.conda\\envs\\NotionGpt\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    137\u001B[0m ):\n\u001B[0;32m    138\u001B[0m     (\n\u001B[0;32m    139\u001B[0m         deployment_id,\n\u001B[0;32m    140\u001B[0m         engine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NotionGpt\\lib\\site-packages\\openai\\api_requestor.py:230\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    211\u001B[0m     method,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    218\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    219\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    220\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[0;32m    221\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m    222\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    228\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[0;32m    229\u001B[0m     )\n\u001B[1;32m--> 230\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[1;32m~\\.conda\\envs\\NotionGpt\\lib\\site-packages\\openai\\api_requestor.py:624\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    616\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    617\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[0;32m    618\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    619\u001B[0m         )\n\u001B[0;32m    620\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[0;32m    621\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 624\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    630\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    631\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\NotionGpt\\lib\\site-packages\\openai\\api_requestor.py:667\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    664\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m OpenAIResponse(\u001B[38;5;28;01mNone\u001B[39;00m, rheaders)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rcode \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m503\u001B[39m:\n\u001B[1;32m--> 667\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mServiceUnavailableError(\n\u001B[0;32m    668\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe server is overloaded or not ready yet.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    669\u001B[0m         rbody,\n\u001B[0;32m    670\u001B[0m         rcode,\n\u001B[0;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrheaders,\n\u001B[0;32m    672\u001B[0m     )\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m rheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "\u001B[1;31mServiceUnavailableError\u001B[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "print(res['choices'][0].message)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## langchain.agents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.agents import create_csv_agent\n",
    "\n",
    "path = \"unzip_data/ADMISSIONS.csv\"\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = create_csv_agent(llm, path, verbose=True)\n",
    "problem = \"from all the csv file, what is the DISCHTIME going to be when his/her ADMITTIME is 2157-10-18 19:34:00 and DIAGNOSIS is INTERIOR MYOCARDIAL INFARCTION?\"\n",
    "prompt = '''\n",
    "I have a problem related to ''' + problem + '''. Could you brainstorm three distinct solutions? Please consider a variety of factors.\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors.\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution.\n",
    "'''\n",
    "print(prompt)\n",
    "agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem = \"from all the csv file, you need to predict what is the DISCHTIME going to be when his/her ADMITTIME is 2157-10-18 19:34:00 and DIAGNOSIS is INTERIOR MYOCARDIAL INFARCTION?\"\n",
    "prompt = '''\n",
    "I have a problem related to ''' + problem + '''. Could you brainstorm three distinct solutions? Please consider a variety of factors.\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors.\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution.\n",
    "'''\n",
    "print(prompt)\n",
    "agent.run(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}